<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources.">
  <meta name="keywords" content="Efficient Multimodal Pre-Training; Fully-Open Multimodal Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><b>ICML 2024</b></span><br> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://victorwz.github.io/">Weizhi Wang</a><sup>1</sup>,&nbsp;&nbsp;
            </span> 
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DxPjkDoAAAAJ&hl=en">Yu Tian</a><sup>2</sup>,&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a><sup>2</sup>,&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://hengcv.github.io/">Heng Wang</a><sup>3</sup>, &nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://sites.cs.ucsb.edu/~xyan/index.htm">Xifeng Yan</a><sup>1</sup>,&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Santa Barbara&nbsp;&nbsp;</span>
            <span class="author-block"><sup>2</sup>Seed Vision Team, ByteDance&nbsp;&nbsp;</span>
            <span class="author-block"><sup>3</sup>Nvidia Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2504.00595"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2504.00595"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/weizhiwang/Open-Qwen2VL"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>Model</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Victorwz/Open-Qwen2VL"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="static/images/CVPR_Teaser_cropped.jpeg" alt="empty">
        <p>
          <br/>
          The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency.
          <br/>The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL.
          <br/> We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methodologies, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: (1) the training codebase, (2) detailed data filtering techniques, and (3) all pre-training and supervised fine-tuning data used to develop the model. 
        </p>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency.
            <br/>The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL.
            <br/> We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: (1) the training codebase, (2) detailed data filtering techniques, and (3) all pre-training and supervised fine-tuning data used to develop the model. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Openness comparisons between SOTA MLLMs and Ours</h2> -->
        <img src="static/images/openness.png" alt="">
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Data Selection, Filtering and Mixture</h2>
        <!-- Interpolating. -->
        <img src="./static/images/sub_dataset.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <img src="./static/images/data_ablations.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          We find that mixing CC3M-CC12M-SBU filtered by CLIP and DataComp-128M filtered by both DFN and MLM-Filter can achieve the best model performance on the multimodal benchmarks.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Multimodal Sequence Packing</h2>

        <!-- Interpolating. -->
        <img src="./static/images/sequence_packing.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          Multimodal sequence packing is vital to reduce the padding tokens and sequence length imbalance. We propose a sequence packing algorithm based on First-fit-decreasing (FFD) bin packing algorithm to significantly enhance the training efficiency.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>

    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Architecture</h2>

        <!-- Interpolating. -->
        <img src="./static/images/model.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          Overview of the proposed model architecture of Open-Qwen2VL. We adopt Adaptive Average Pooling layer in the pre-training stage to project 729 image patch ouputs from SigLIP vision encoder to 144 image tokens, then followed by MLP layer. In the SFT stage, the AveragePooling is discarded and only MLP layer is used.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>

    <!--/ Method. -->
    <br />
    <!-- Results -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <h3 class="title is-4">1. Final Performance Comparisons with SOTA Close-Source MLLMs</h3>
        <img src="./static/images/final_performance.png" class="interpolation-image" alt="Empty"/>
        <br />
        <p>
          We compare Open-Qwen2VL with SOTA 2B-parameter models of InternVL-2.5-2B-MPO, DeepSeekVL-2-Tiny, and Qwen2-VL-2B-Instruct. Our model outperforms Qwen2-VL-2B-Instruct on most of benchmarks, while it is trained on only 0.36% tokens of Qwen2-VL-2B-Instruct. Our model achieves incredible pre-training efficiency.
        </p>
        <br/>
        <h3 class="title is-4">2. Scaling Effects of Visual SFT</h3>
        <img width="1000px" src="./static/images/scaling_sft.png" class="interpolation-image" alt="Empty"/>
        <p>
          <br />
          We adopt <a href="https://mammoth-vl.github.io/">MAmmoTH-VL-Single-Image-10M</a> sft data and evaluate the model checkpoints every 2M examples. We find that scaled up post-trianing significantly enhance the model performance on multimodal benchmarks. 
        </p>
        <br/>
        <h3 class="title is-4">3. Cases</h3>
        <img width="1000px" src="./static/images/case1.png" class="interpolation-image" alt="Empty"/><br/>
        <img width="1000px" src="./static/images/case2.png" class="interpolation-image" alt="Empty"/>
        <!-- <p>
          <br />
          Visualization of Zero-Shot 2D Animation Generation. MagicPose can provide a precise generation with identity information from cartoon-style images even without any further fine-tuning after being trained on real-human dance videos.  
        </p> -->
      </div>
    </div>


</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{Open-Qwen2VL,
    title={Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources},
    author={Wang, Weizhi and Tian, Yu and Yang, Linjie and Wang, Heng and Yan, Xifeng},
    journal={arXiv preprint arXiv:2504.00595},
    year={2025}
  }</code></pre>
  </div>
</section>


<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      We would like to thank for Facebook (now Meta) for donating the 8xA100-40G GPUs for conducting the experiments. We appreciate the codebase of <a href="https://github.com/TRI-ML/prismatic-vlms/tree/main">prismatic-vlms</a> and <a href="https://github.com/TRI-ML/vlm-evaluation">vlm-evaluation</a>, on which we build our codebase.
    </p>
  </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
